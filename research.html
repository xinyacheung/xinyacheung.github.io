---
layout: page
title: Research
permalink: /research/
---

<font size="+2"><strong> Publications</strong></font><br>
<div class="textLayer"><span dir="ltr" role="presentation">1.</span> <a href="https://www.biorxiv.org/content/10.1101/2023.12.01.569615v1">Adaptive stretching of representations across brain regions and deep learning model layers. </a></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">Xin-Ya Zhang</span></strong><span dir="ltr" role="presentation">, Sebastian Bobadilla-Suarez, Xiaoliang Luo, Marilena Lemonari, Scott L. Brincat, Markus Siegel, Earl K. Miller, Bradley C. Love. <em>bioRxiv.</em><br /></span></span><span dir="ltr" role="presentation">Prefrontal cortex (PFC) is known to modulate the visual system to favor goal-relevant information by accentuating task-relevant stimulus dimensions. Does the brain broadly re-configures itself to optimize performance by stretching visual representations along task-relevant dimensions? We considered a task that required monkeys to selectively attend on a trial-by-trial basis to one of two dimensions (color or motion direction) to make a decision. Except for V4 (color bound) and MT (motion bound), the brain radically reconfigured itself to stretch representations along task-relevant dimensions in lateral PFC, frontal eye fields (FEF), lateral intraparietal cortex (LIP), and inferotemporal cortex (IT). Spike timing was crucial to this code. A deep learning model was trained on the same visual input and rewards as the monkeys. Despite lacking an explicit selective attention or other control mechanism, the model displayed task-relevant stretching as a consequence of error minimization, indicating that stretching is an adaptive strategy.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">2.</span> <a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.L032045">Why temporal networks are more controllable: Link weight variation offers superiority </a></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, J. Sun and G. Yan. </span><em><span dir="ltr" role="presentation">Physical Review Research</span><span dir="ltr" role="presentation">, 3(3), p.L032045. 2021.</span></em><br role="presentation" /><span dir="ltr" role="presentation">We explored a general model of temporal networks and analytically proved that the weight </span><span dir="ltr" role="presentation">variation of a link is equivalent to attaching a virtual driver node to that link. Consequently, </span><span dir="ltr" role="presentation">the temporality of link weights can significantly increase the dimension of controllable space and </span><span dir="ltr" role="presentation">remarkably reduce control cost.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">3.</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26152">Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect. </a></div>
    <!-- &nbsp;&nbsp;</span></div> -->
<div class="textLayer"><span dir="ltr" role="presentation">X. Ru,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation"><span dir="ltr" role="presentation">, Zijia Liu, J. M. Moore and G. Yan. <em>NeurIPS 2023.</em><br /></span></span><span dir="ltr" role="presentation">We consider the problem of reconstructing coupled networks (e.g., biological neural networks) connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak coupling-drive. The core difficulty is sparseness of coupling effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in time series (e.g., neuronal activity sequence). Here we learn the idea from attention mechanism to guide the classifier to make inference focusing on the critical regions of time series data where coupling effect may manifest. Specifically, attention coefficients are assigned autonomously by artificial neural networks trained to maximise the Attentive Transfer Entropy (ATEn), which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real directed coupling networks using data generated by neuronal models widely used in neuroscience.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">4.</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26152">Inferring Patient Zero on Temporal Networks via Graph Neural Networks </a></div>
<div class="textLayer"><span dir="ltr" role="presentation">X. Ru, J. M. Moore,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, Y. Zeng and G. Yan. </span><span dir="ltr" role="presentation"><em>AAAI 2023.</em></span><br role="presentation" /><span dir="ltr" role="presentation">The world is currently seeing frequent local outbreaks of epidemics, such as COVID-19 and </span><span dir="ltr" role="presentation">Monkeypox.</span> <span dir="ltr" role="presentation">Preventing further propagation of the outbreak requires prompt implementation </span><span dir="ltr" role="presentation">of control measures, and a critical step is to quickly infer patient zero. To address these chal</span><span dir="ltr" role="presentation">lenges, we tailor a GNN-based model to establish the inverse statistical association between the </span><span dir="ltr" role="presentation">current and initial state implicitly. We also demonstrate that our method is robust to missing </span><span dir="ltr" role="presentation">information about contact structure or current state.</span></div>
<br>
<font size="+2"><strong> Research experience</strong></font><br>
<div class="textLayer"><span dir="ltr" role="presentation"><br role="presentation" />Link weight variation offers superiority in controlling temporal networks 2022<br role="presentation" />Oral presentation at International Conference NetSci 2022. We found that degree-heterogeneous networks are more advantageous for enhancing controllability, and the favorable positions of weight variation are the incoming links of the nodes with a high outdegree and a low indegree, as well as the topological position of the perturbed link plays a crucial role in such an effect. The control cost increases approximately exponentially with the distance from the driver node to the perturbed link.</span></div>
<div class="textLayer">&nbsp;</div>
<div class="textLayer"><span dir="ltr" role="presentation">Structural origin of co-susceptibility in cascading failures 2018<br role="presentation" />Oral presentation at International Conference NetSci-X 2018. We found that both structural closeness and high-order correlations could lead to co-susceptibility. This finding prompted us to propose a new statistical quantity, based on structure only, to assess the co-susceptibility of node pairs in an arbitrary network.</span></div>
<div class="textLayer">&nbsp;</div>
<br>
<font size="+2"><strong> Working Papers</strong></font><br>
<div class="textLayer"><span dir="ltr" role="presentation" />&bull; Data-driven control of nonlinear network systems<br role="presentation" />While most previous work of network control focused on linearized systems that represent local dynamics around a basin, real network systems exhibit multiple basins or attractors. Hence it is of importance to find the strategy to steer the network system from one basin to any desired basin. In this project we are developing a deep learning framework for controlling nonlinear systems. The method is model-free, i.e. based on the observation of nodal activities only.</span></div>
<br>