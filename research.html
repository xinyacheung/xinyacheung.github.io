---
layout: page
title: Research
permalink: /research/
---

<font size="+2"><strong> Publications</strong></font><br>
<div class="textLayer"><a href="https://scholar.google.com/citations?user=C9GFvD0AAAAJ&hl=en">Google Scholar</a></div> 

<div class="textLayer"><span dir="ltr" role="presentation">1.</span> <a href="https://www.biorxiv.org/content/10.1101/2023.12.01.569615v1">Adaptive stretching of representations across brain regions and deep learning model layers </a></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">Xin-Ya Zhang</span></strong><span dir="ltr" role="presentation">, S. Bobadilla-Suarez, X. Luo, M. Lemonari, S. L. Brincat, M. Siegel, E. K. Miller and B. C. Love. <em>bioRxiv.</em><br /></span></span><span dir="ltr" role="presentation">We found that except for V4 (color bound) and MT (motion bound), the brain radically reconfigured itself to stretch representations along task-relevant dimensions in multiple regions. A deep learning model (CNN-LSTM) was trained on the same visual input and rewards as the monkeys. Despite lacking an explicit selective attention or other control mechanism, the model displayed task-relevant stretching as a consequence of error minimization, indicating that stretching is an adaptive strategy.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">2.</span> <a href="https://www.biorxiv.org/content/10.1101/2024.04.02.587853v1">Decoding region-level visual functions from invasive EEG data </a></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">Xin-Ya Zhang</span></strong><span dir="ltr" role="presentation">, H. Lin, Z. Deng, M. Siegel, E. K. Miller, G. Yan. <em>bioRxiv.</em><br /></span></span><span dir="ltr" role="presentation">When we encounter familiar environments, the brain may trigger neuronal activities that resemble specific patterns. Our findings show that the machine effectively captures the function of individual brain regions as a consequence of minimizing visual errors. Specifically, in areas related to vision, V4 and LIP are implicated in visual color and shape processing, while MT is more involved in visual motion. Moreover, we highlight the importance of the source brain region in both decoding and encoding processes. While each brain region harbors some global information, achieving good performance in vision reconstruction necessitates utilizing data from regions closely linked with vision processing.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">3.</span>Geometric scaling law in real neuronal networks </div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">Xin-Ya Zhang</span></strong><span dir="ltr" role="presentation">, J. M. Moore, X. Ru and G. Yan. <em>submitted.</em><br /></span></span><span dir="ltr" role="presentation">We uncovered a fundamental and elegant scaling law in the synapse-resolution Drosophila connectomes. This discovery challenges the well-known exponential distance rule previously established in inter-areal brain networks and carries functional significance, aligning with the maximum entropy of information communication and the functional criticality balancing integration and segregation. Our findings establish a direct link between brain geometry and topology, hinting at new opportunities for developing brain geometry-inspired artificial intelligence.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">4.</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26152">Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect </a></div>
<div class="textLayer"><span dir="ltr" role="presentation">X. Ru,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation"><span dir="ltr" role="presentation">, Zijia Liu, J. M. Moore and G. Yan. <em>NeurIPS 2023.</em><br /></span></span><span dir="ltr" role="presentation">We consider the problem of reconstructing coupled networks (e.g., biological neural networks) connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak coupling-drive. The core difficulty is sparseness of coupling effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in time series (e.g., neuronal activity sequence). Here we learn the idea from attention mechanism to guide the classifier to make inference focusing on the critical regions of time series data where coupling effect may manifest. Specifically, attention coefficients are assigned autonomously by artificial neural networks trained to maximise the Attentive Transfer Entropy (ATEn), which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real directed coupling networks using data generated by neuronal models widely used in neuroscience.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">5.</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26152">Inferring Patient Zero on Temporal Networks via Graph Neural Networks </a></div>
<div class="textLayer"><span dir="ltr" role="presentation">X. Ru, J. M. Moore,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, Y. Zeng and G. Yan. </span><span dir="ltr" role="presentation"><em>AAAI 2023.</em></span><br role="presentation" /><span dir="ltr" role="presentation">The world is currently seeing frequent local outbreaks of epidemics, such as COVID-19 and </span><span dir="ltr" role="presentation">Monkeypox.</span> <span dir="ltr" role="presentation">Preventing further propagation of the outbreak requires prompt implementation </span><span dir="ltr" role="presentation">of control measures, and a critical step is to quickly infer patient zero. To address these chal</span><span dir="ltr" role="presentation">lenges, we tailor a GNN-based model to establish the inverse statistical association between the </span><span dir="ltr" role="presentation">current and initial state implicitly. We also demonstrate that our method is robust to missing </span><span dir="ltr" role="presentation">information about contact structure or current state.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">6.</span> <a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.L032045">Why temporal networks are more controllable: Link weight variation offers superiority </a></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, J. Sun and G. Yan. </span><em><span dir="ltr" role="presentation">Physical Review Research</span><span dir="ltr" role="presentation">, 3(3), p.L032045. 2021.</span></em><br role="presentation" /><span dir="ltr" role="presentation">We explored a general model of temporal networks and analytically proved that the weight </span><span dir="ltr" role="presentation">variation of a link is equivalent to attaching a virtual driver node to that link. Consequently, </span><span dir="ltr" role="presentation">the temporality of link weights can significantly increase the dimension of controllable space and </span><span dir="ltr" role="presentation">remarkably reduce control cost.</span></div>
<br>

<div class="section-title">Talks</div>
    
<div class="textLayer">
    <span class="event-item-title">Geometric scaling law in real neuronal networks</span><br />
    <span class="event-item-description">Oral presentation at 20th China Networks Science Forum 2024.</span>
</div>

<div class="textLayer">
    <span class="event-item-title">Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect</span><br />
    <span class="event-item-description">Spotlight at International Conference NeurIPS 2023.</span>
</div>

<div class="textLayer">
    <span class="event-item-title">Link weight variation offers superiority in controlling temporal networks</span><br />
    <span class="event-item-description">Oral presentation at International Conference NetSci 2022.</span>
</div>

<div class="textLayer">
    <span class="event-item-title">Structural origin of co-susceptibility in cascading failures</span><br />
    <span class="event-item-description">Oral presentation at International Conference NetSci-X 2018. We found that both structural closeness and high-order correlations could lead to co-susceptibility. This finding prompted us to propose a new statistical quantity, based on structure only, to assess the co-susceptibility of node pairs in an arbitrary network.</span>
</div>

