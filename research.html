---
layout: page
title: Research
permalink: /research/
---

<font size="+2"><strong> <a href="https://scholar.google.com/citations?user=C9GFvD0AAAAJ&hl=en">Publications</a></strong></font><br>
<br>
<p><strong>Journal papers</strong></p>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>1.</span> <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.133.138401">Geometric Scaling Law in Real Neuronal Networks </a></strong></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, J. M. Moore, X. Ru and G. Yan. <em>Physical Review Letters</em>, 2024. Editors' Suggestion & Featured in Physics<br /></span></span><span dir="ltr" role="presentation">We uncovered a fundamental and elegant scaling law in the synapse-resolution Drosophila connectomes. This discovery challenges the well-known exponential distance rule previously established in inter-areal brain networks and carries functional significance, aligning with the maximum entropy of information communication and the functional criticality balancing integration and segregation. Our findings establish a direct link between brain geometry and topology, hinting at new opportunities for developing brain geometry-inspired artificial intelligence.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>2.</span> <a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.L032045">Why Temporal Networks Are More Controllable: Link Weight Variation Offers Superiority </a></strong></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, J. Sun and G. Yan. <em>Physical Review Research</em>, 2021.<br /></span></span><span dir="ltr" role="presentation">We explored a general model of temporal networks and analytically proved that the weight </span><span dir="ltr" role="presentation">variation of a link is equivalent to attaching a virtual driver node to that link. Consequently, </span><span dir="ltr" role="presentation">the temporality of link weights can significantly increase the dimension of controllable space and </span><span dir="ltr" role="presentation">remarkably reduce control cost.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>3.</span> <a href="https://ieeexplore.ieee.org/abstract/document/10723746">Mixup in Latent Geometry for Graph Classification </a></strong></div>
<div class="textLayer"><span dir="ltr" role="presentation">Z. Liu, X. Ru, J. M. Moore,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation"> and G. Yan. <em>IEEE Transactions on Network Science and Engineering</em>, 2024.<br /></span></span><span dir="ltr" role="presentation">Mixup is relatively straightforward to apply to image samples because pixels with equivalent coordinates in different images can be associated. However, alignment of distinct graphs with different sizes is non-trivial, thereby hindering the application of Mixup to graph data. Here we develop a novel algorithm to address this issue by exploiting the latent hyperbolic geometry which has been shown to underlie many real-world graphs.</span></div>
<br>
<p><strong>Conference papers</strong></p>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>1.</span> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/00bb4e415ef117f2dee2fc3b778d806d-Abstract-Conference.html">Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect </a></strong></div>
<div class="textLayer"><span dir="ltr" role="presentation">X. Ru,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation"><span dir="ltr" role="presentation">, Z. Liu, J. M. Moore and G. Yan. <em>NeurIPS</em>, 2023. Spotlight<br /></span></span><span dir="ltr" role="presentation">We consider the problem of reconstructing coupled networks (e.g., biological neural networks) connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak coupling-drive. We learn the idea from attention mechanism to guide the classifier to make inference focusing on the critical regions of time series data where coupling effect may manifest. Our results show that, without any prior knowledge of dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. </span></div>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>2.</span> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26152">Inferring Patient Zero on Temporal Networks via Graph Neural Networks </a></strong></div>
<div class="textLayer"><span dir="ltr" role="presentation">X. Ru, J. M. Moore,</span> <strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, Y. Zeng and G. Yan. </span><span dir="ltr" role="presentation"><em>AAAI</em>, 2023. Oral</span><br role="presentation" /><span dir="ltr" role="presentation">The world is currently seeing frequent local outbreaks of epidemics, such as COVID-19 and </span><span dir="ltr" role="presentation">Monkeypox.</span> <span dir="ltr" role="presentation">Preventing further propagation of the outbreak requires prompt implementation </span><span dir="ltr" role="presentation">of control measures, and a critical step is to quickly infer patient zero. To address these chal</span><span dir="ltr" role="presentation">lenges, we tailor a GNN-based model to establish the inverse statistical association between the </span><span dir="ltr" role="presentation">current and initial state implicitly. We also demonstrate that our method is robust to missing </span><span dir="ltr" role="presentation">information about contact structure or current state.</span></div>
<br>
<p><strong>Preprints</strong></p>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>1.</span> <a href="https://www.biorxiv.org/content/10.1101/2023.12.01.569615v1">Adaptive stretching of representations across brain regions and deep learning model layers </a></strong></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">X.-Y. Zhang</span></strong><span dir="ltr" role="presentation">, S. Bobadilla-Suarez, X. Luo, M. Lemonari, S. L. Brincat, M. Siegel, E. K. Miller and B. C. Love. <em>bioRxiv.</em><br /></span></span><span dir="ltr" role="presentation">We found that except for V4 (color bound) and MT (motion bound), the brain radically reconfigured itself to stretch representations along task-relevant dimensions in multiple regions. A deep learning model (CNN-LSTM) was trained on the same visual input and rewards as the monkeys. Despite lacking an explicit selective attention or other control mechanism, the model displayed task-relevant stretching as a consequence of error minimization, indicating that stretching is an adaptive strategy.</span></div>
<div class="textLayer"><span dir="ltr" role="presentation">
    <strong>2.</span> <a href="https://www.biorxiv.org/content/10.1101/2024.04.02.587853v2.full">Decoding region-level visual functions from invasive EEG data </a></strong></div>
<div class="textLayer"><strong><span dir="ltr" role="presentation">X.-Y. Zhang<sup>†</sup></span></strong><span dir="ltr" role="presentation">, H. Lin<sup>†</sup>, Z. Deng, M. Siegel, E. K. Miller, G. Yan. <em>bioRxiv.</em><br /></span></span><span dir="ltr" role="presentation">When we encounter familiar environments, the brain may trigger neuronal activities that resemble specific patterns. Our findings show that the machine effectively captures the function of individual brain regions as a consequence of minimizing visual errors. Specifically, in areas related to vision, V4 and LIP are implicated in visual color and shape processing, while MT is more involved in visual motion. Moreover, we highlight the importance of the source brain region in both decoding and encoding processes. While each brain region harbors some global information, achieving good performance in vision reconstruction necessitates utilizing data from regions closely linked with vision processing.</span></div>
<br>
<p><strong>Working</strong></p>
<ul>
    <li>Organization of a neuron-resolution central brain network: Topology, geometry and anatomy.</li>
    <li>Delayed threshold and spatial diffusion in k-core percolation induced by long-range connectivity.</li>
</ul>
<br>
<br>
<font size="+2"><strong> Presentations </strong></font><br>
<ul>
    <li><strong>Poster presentation at the 17th Annual Meeting of Chinese Neuroscience Society 2024, Suzhou, China.</strong><br>Adaptive stretching of representations across brain regions and deep learning model layers.</li>
    <li><strong>Oral presentation at 20th China Networks Science Forum 2024, Beijing, China.</strong><br> Geometric scaling law in real neuronal networks.</li>
    <li><strong>Spotlight at International Conference NeurIPS 2023, New Orlean, USA.</strong><br>Attentive transfer entropy to exploit transient emergence of coupling effect.</li>
    <li><strong>Oral presentation at International Conference NetSci 2022, Shanghai, China.</strong><br>Link weight variation offers superiority in controlling temporal networks.</li>
    <li><strong>Oral presentation at International Conference NetSci-X 2018, Hangzhou, China.</strong><br>Structural origin of co-susceptibility in cascading failures. We found that both structural closeness and high-order correlations could lead to co-susceptibility. This finding prompted us to propose a new statistical quantity, based on structure only, to assess the co-susceptibility of node pairs in an arbitrary network.</li>
</ul>


